# -*- coding: utf-8 -*-
"""ODE BY NN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YbiTIIb9b6udW-bzLLxP-jyw5PWrCia9
"""

#practice code 1
#importing libraries
import autograd.numpy as np
from autograd import grad, elementwise_grad
import autograd.numpy.random as npr
from matplotlib import pyplot as plt

#defining the activation function
def sigmoid(z):
    return 1/(1+np.exp(-z))

#defining neural netwrok
def NN(para,x):
    w_h1=para[0]
    w_h2=para[1]
    w_out=para[2]

    #size of input
    size=np.size(x)
    x= x.reshape(-1,size)

    #getting input
    input=x

    #work in hidden layer 1
    #adding row in our input
    input_modified=np.concatenate((np.ones((1,size)),input),axis=0)
    #input and output of hidden layer 1
    h1_in=np.matmul(w_h1,input_modified)
    h1_output=sigmoid(h1_in)

    #work in hidden layer 2
    #add bias row
    h1_out_modified=np.concatenate((np.ones((1,size)),h1_output),axis=0)
    #input and output of hidden layer 2
    h2_in=np.matmul(w_h2, h1_out_modified)
    h2_out=sigmoid(h2_in)

    #work in output layer
    #add bias row
    h2_out_modified=np.concatenate((np.ones((1,size)),h2_out),axis=0)
    #input and output of output layer
    outlayer_in=np.matmul(w_out,h2_out_modified)
    outlayer_out=outlayer_in
    return outlayer_out

#now trial solution
def g_trial(x,para,g0=10):
    return g0+x*NN(para,x)

#RHS of ODE
def g(x,g_trial,gamma=2):
    return -gamma*g_trial

#cost function
def cost_function(P,x):
    #put parameters in trial solution
    g_t=g_trial(x,P)

    #finding derivative of NN element wise
    d_NN= elementwise_grad(NN,1)(P,x)

    #finfing derivative of trial function
    d_g_t=elementwise_grad(g_trial,0)(x,P)

    #THE RIGHT SIDE OF ODE
    func=g(x,g_t)
    err_sqr=(d_g_t -func)**2
    cost_sum=np.sum(err_sqr)
    return cost_sum/np.size(err_sqr)

#solving the ode
def solve_ode (x,N_h1,N_h2,iter,lmb):
    #setup the weights and biasis

    #for hidden layer 1
    p0=npr.randn(N_h1,2)
    #for hidden layer 2
    p1=npr.randn(N_h2,N_h1+1)
    #for output layer
    p2=npr.randn(1,N_h2+1)
    P=[p0,p1,p2]
    print('Initial cost: %g'%cost_function(P, x))

    cost_function_gradient=grad(cost_function,0)
    for i in range (iter):
        cost_grad=cost_function_gradient(P,x)
        P[0]=P[0]-lmb*cost_grad[0]
        P[1]=P[1]-lmb*cost_grad[1]
        P[2]=P[2]-lmb*cost_grad[2]
    print('final cost:%g'%cost_function(P,x))
    return P

def g_analytic(x,gamma=2,g0=10):
    return g0*np.exp(-gamma*x)

if __name__=='__main__':
    npr.seed(15)
    N=10
    x=np.linspace(0,1,N)
    N_h1=10
    N_h2=20
    iter=1000
    lmb=0.001

    #use the network
    P=solve_ode (x,N_h1,N_h2,iter,lmb)

    res=g_trial(x,P)
    res_analytical=g_analytic(x)
    print('Max absloute difference :%g'%np.max(np.abs(res-res_analytical)))

    #plotting
    plt.figure(figsize=(5,5))
    plt.title('performance of neural network')
    plt.plot(x, res_analytical)
    plt.plot(x, res[0,:])
    plt.legend(['analytical','nn'])
    plt.xlabel('x')
    plt.ylabel('g(x)')
    plt.show()

#code by sir
import autograd.numpy as np
from autograd import grad, elementwise_grad
import autograd.numpy.random as npr
from matplotlib import pyplot as plt

def sigmoid(z):
    return 1/(1 + np.exp(-z))

# Assuming one input, hidden, and output layer
def neural_network(params, x):

    # Find the weights (including and biases) for the hidden and output layer.
    # Assume that params is a list of parameters for each layer.
    # The biases are the first element for each array in params,
    # and the weights are the remaning elements in each array in params.

    w_hidden = params[0]
    w_output = params[1]

    # Assumes input x being an one-dimensional array
    num_values = np.size(x)
    x = x.reshape(-1, num_values)

    # Assume that the input layer does nothing to the input x
    x_input = x

    ## Hidden layer:

    # Add a row of ones to include bias
    x_input = np.concatenate((np.ones((1,num_values)), x_input ), axis = 0)

    z_hidden = np.matmul(w_hidden, x_input)
    x_hidden = sigmoid(z_hidden)

    ## Output layer:

    # Include bias:
    x_hidden = np.concatenate((np.ones((1,num_values)), x_hidden ), axis = 0)

    z_output = np.matmul(w_output, x_hidden)
    x_output = z_output

    return x_output

# The trial solution using the deep neural network:
def g_trial(x,params, g0 = 10):
    return g0 + x*neural_network(params,x)

# The right side of the ODE:
def g(x, g_trial, gamma = 2):
    return -gamma*g_trial

# The cost function:
def cost_function(P, x):

    # Evaluate the trial function with the current parameters P
    g_t = g_trial(x,P)

    # Find the derivative w.r.t x of the neural network
    d_net_out = elementwise_grad(neural_network,1)(P,x)

    # Find the derivative w.r.t x of the trial function
    d_g_t = elementwise_grad(g_trial,0)(x,P)

    # The right side of the ODE
    func = g(x, g_t)

    err_sqr = (d_g_t - func)**2
    cost_sum = np.sum(err_sqr)

    return cost_sum / np.size(err_sqr)

# Solve the exponential decay ODE using neural network with one input, hidden, and output layer
def solve_ode_neural_network(x, num_neurons_hidden, num_iter, lmb):
    ## Set up initial weights and biases

    # For the hidden layer
    p0 = npr.randn(num_neurons_hidden, 2 )

    # For the output layer
    p1 = npr.randn(1, num_neurons_hidden + 1 ) # +1 since bias is included

    P = [p0, p1]

    print('Initial cost: %g'%cost_function(P, x))

    ## Start finding the optimal weights using gradient descent

    # Find the Python function that represents the gradient of the cost function
    # w.r.t the 0-th input argument -- that is the weights and biases in the hidden and output layer
    cost_function_grad = grad(cost_function,0)

    # Let the update be done num_iter times
    for i in range(num_iter):
        # Evaluate the gradient at the current weights and biases in P.
        # The cost_grad consist now of two arrays;
        # one for the gradient w.r.t P_hidden and
        # one for the gradient w.r.t P_output
        cost_grad =  cost_function_grad(P, x)

        P[0] = P[0] - lmb * cost_grad[0]
        P[1] = P[1] - lmb * cost_grad[1]

    print('Final cost: %g'%cost_function(P, x))

    return P

def g_analytic(x, gamma = 2, g0 = 10):
    return g0*np.exp(-gamma*x)

# Solve the given problem
if __name__ == '__main__':
    # Set seed such that the weight are initialized
    # with same weights and biases for every run.
    npr.seed(15)

    ## Decide the vales of arguments to the function to solve
    N = 10
    x = np.linspace(0, 1, N)

    ## Set up the initial parameters
    num_hidden_neurons = 10
    num_iter = 10000
    lmb = 0.001

    # Use the network
    P = solve_ode_neural_network(x, num_hidden_neurons, num_iter, lmb)

    # Print the deviation from the trial solution and true solution
    res = g_trial(x,P)
    res_analytical = g_analytic(x)

    print('Max absolute difference: %g'%np.max(np.abs(res - res_analytical)))

    # Plot the results
    plt.figure(figsize=(4,4))

    plt.title('Performance of neural network solving an ODE compared to the analytical solution')
    plt.plot(x, res_analytical)
    plt.plot(x, res[0,:])
    plt.legend(['analytical','nn'])
    plt.xlabel('x')
    plt.ylabel('g(x)')
    plt.show()



#solving first equation population

#importing libraries
import autograd.numpy as np
from autograd import grad, elementwise_grad
import autograd.numpy.random as npr
from matplotlib import pyplot as plt

#defining the activation function
def sigmoid(z):
    return 1/(1+np.exp(-z))

#defining neural netwrok
def NN(para,x):
    w_h1=para[0]
    w_h2=para[1]
    w_out=para[2]

    #size of input
    size=np.size(x)
    x= x.reshape(-1,size)

    #getting input
    input=x

    #work in hidden layer 1
    #adding row in our input
    input_modified=np.concatenate((np.ones((1,size)),input),axis=0)
    #input and output of hidden layer 1
    h1_in=np.matmul(w_h1,input_modified)
    h1_output=sigmoid(h1_in)

    #work in hidden layer 2
    #add bias row
    h1_out_modified=np.concatenate((np.ones((1,size)),h1_output),axis=0)
    #input and output of hidden layer 2
    h2_in=np.matmul(w_h2, h1_out_modified)
    h2_out=sigmoid(h2_in)

    #work in output layer
    #add bias row
    h2_out_modified=np.concatenate((np.ones((1,size)),h2_out),axis=0)
    #input and output of output layer
    outlayer_in=np.matmul(w_out,h2_out_modified)
    outlayer_out=outlayer_in
    return outlayer_out

#now trial solution
def g_trial(x,para,g0=10):
    return g0+x*NN(para,x)

#RHS of ODE
def g(x,g_trial,gamma=2):
    return gamma*g_trial

#cost function
def cost_function(P,x):
    #put parameters in trial solution
    g_t=g_trial(x,P)

    #finding derivative of NN element wise
    d_NN= elementwise_grad(NN,1)(P,x)

    #finfing derivative of trial function
    d_g_t=elementwise_grad(g_trial,0)(x,P)

    #THE RIGHT SIDE OF ODE
    func=g(x,g_t)
    err_sqr=(d_g_t -func)**2
    cost_sum=np.sum(err_sqr)
    return cost_sum/np.size(err_sqr)

#solving the ode
def solve_ode (x,N_h1,N_h2,iter,lmb):
    #setup the weights and biasis

    #for hidden layer 1
    p0=npr.randn(N_h1,2)
    #for hidden layer 2
    p1=npr.randn(N_h2,N_h1+1)
    #for output layer
    p2=npr.randn(1,N_h2+1)
    P=[p0,p1,p2]
    print('Initial cost: %g'%cost_function(P, x))

    cost_function_gradient=grad(cost_function,0)
    for i in range (iter):
        cost_grad=cost_function_gradient(P,x)
        P[0]=P[0]-lmb*cost_grad[0]
        P[1]=P[1]-lmb*cost_grad[1]
        P[2]=P[2]-lmb*cost_grad[2]
    print('final cost:%g'%cost_function(P,x))
    return P

def g_analytic(x,gamma=2,g0=10):
    return g0 * np.exp(gamma*x)

if __name__=='__main__':
    npr.seed(15)
    N=10
    x=np.linspace(0,1,N)
    N_h1=50
    N_h2=50
    iter=1000
    lmb=0.001

    #use the network
    P=solve_ode (x,N_h1,N_h2,iter,lmb)

    res=g_trial(x,P)
    res_analytical=g_analytic(x)
    print('Max absloute difference :%g'%np.max(np.abs(res-res_analytical)))

    #plotting
    plt.figure(figsize=(5,5))
    plt.title('performance of neural network')
    plt.plot(x, res_analytical)
    plt.plot(x, res[0,:])
    plt.legend(['analytical','nn'])
    plt.xlabel('x')
    plt.ylabel('g(x)')
    plt.show()

#solving first fluid problem 1D

#importing libraries
import autograd.numpy as np
from autograd import grad, elementwise_grad
import autograd.numpy.random as npr
from matplotlib import pyplot as plt

#defining the activation function
def sigmoid(z):
    return 1/(1+np.exp(-z))

#defining neural netwrok
def NN(para,x):
    w_h1=para[0]
    w_h2=para[1]
    w_out=para[2]

    #size of input
    size=np.size(x)
    x= x.reshape(-1,size)

    #getting input
    input=x

    #work in hidden layer 1
    #adding row in our input
    input_modified=np.concatenate((np.ones(1,size),input),axis=0)
    #input and output of hidden layer 1
    h1_in=np.matmul(w_h1,input_modified)
    h1_output=sigmoid(h1_in)

    #work in hidden layer 2
    #add bias row
    h1_out_modified=np.concatenate((np.ones((1,size)),h1_output),axis=0)
    #input and output of hidden layer 2
    h2_in=np.matmul(w_h2, h1_out_modified)
    h2_out=sigmoid(h2_in)

    #work in output layer
    #add bias row
    h2_out_modified=np.concatenate((np.ones((1,size)),h2_out),axis=0)
    #input and output of output layer
    outlayer_in=np.matmul(w_out,h2_out_modified)
    outlayer_out=outlayer_in
    return outlayer_out

#now trial solution
def g_trial(x,para,g0=10):
    return g0+x*NN(para,x)

#RHS of ODE
def g(x,g_trial,gamma=2):
    return 10 - gamma*g_trial

#cost function
def cost_function(P,x):
    #put parameters in trial solution
    g_t=g_trial(x,P)

    #finding derivative of NN element wise
    d_NN= elementwise_grad(NN,1)(P,x)

    #finfing derivative of trial function
    d_g_t=elementwise_grad(g_trial,0)(x,P)

    #THE RIGHT SIDE OF ODE
    func=g(x,g_t)
    err_sqr=(d_g_t -func)**2
    cost_sum=np.sum(err_sqr)
    return cost_sum/np.size(err_sqr)

#solving the ode
def solve_ode (x,N_h1,N_h2,iter,lmb):
    #setup the weights and biasis

    #for hidden layer 1
    p0=npr.randn(N_h1,2)
    #for hidden layer 2
    p1=npr.randn(N_h2,N_h1+1)
    #for output layer
    p2=npr.randn(1,N_h2+1)
    P=[p0,p1,p2]
    print('Initial cost: %g'%cost_function(P, x))

    cost_function_gradient=grad(cost_function,0)
    for i in range (iter):
        cost_grad=cost_function_gradient(P,x)
        P[0]=P[0]-lmb*cost_grad[0]
        P[1]=P[1]-lmb*cost_grad[1]
        P[2]=P[2]-lmb*cost_grad[2]
    print('final cost:%g'%cost_function(P,x))
    return P

def g_analytic(x,gamma=0.025,g0=40):
    return  400 -360*np.exp(-0.025*x)

if __name__=='__main__':
    npr.seed(15)
    N=10
    x=np.linspace(0,1,N)
    N_h1=50
    N_h2=50
    iter=1000
    lmb=0.001

    #use the network
    P=solve_ode (x,N_h1,N_h2,iter,lmb)

    res=g_trial(x,P)
    res_analytical=g_analytic(x)
    print('Max absloute difference :%g'%np.max(np.abs(res-res_analytical)))

    #plotting
    plt.figure(figsize=(5,5))
    plt.title('performance of neural network')
    plt.plot(x, res_analytical)
    plt.plot(x, res[0,:])
    plt.legend(['analytical','nn'])
    plt.xlabel('x')
    plt.ylabel('g(x)')
    plt.show()

#financial math problems 1st equation
import autograd.numpy as np
from autograd import grad, elementwise_grad
import autograd.numpy.random as npr
from matplotlib import pyplot as plt

def sigmoid(z):
  return 1/(1+np.exp(-z))

def neural_network(params, x):
    # Find the weights (including and biases) for the hidden and output layer.
    # Assume that params is a list of parameters for each layer.
    # The biases are the first element for each array in params,
    # and the weights are the remaining elements in each array in params.

    w_hidden = params[0]
    w_output = params[1]

    # Assumes input x being a one-dimensional array
    num_values = np.size(x)
    x = x.reshape(-1, num_values)

    # Assume that the input layer does nothing to the input x
    x_input = x

    ## Hidden layer:

    # Add a row of ones to include bias
    x_input = np.concatenate((np.ones((1, num_values)), x_input), axis=0)

    z_hidden = np.matmul(w_hidden, x_input)
    x_hidden = sigmoid(z_hidden)

    ## Output layer:

    # Include bias:
    x_hidden = np.concatenate((np.ones((1, num_values)), x_hidden), axis=0)

    z_output = np.matmul(w_output, x_hidden)
    x_output = z_output

    return x_output


# Define the right-hand side of the non-homogeneous ODE
def g(t, P):
    r = 1  # Given constant r
    return r * P + 2

# Define the trial solution using the neural network
def g_trial(t, params, P0=3):
    return P0 + t * neural_network(params, t)

# Define the cost function
def cost_function(params, t):
    g_t = g_trial(t, params)
    d_g_t = elementwise_grad(g_trial, 0)(t, params)
    func = g(t, g_t)
    err_sqr = (d_g_t - func)**2
    return np.mean(err_sqr)

def solve_ode_neural_network(t, num_neurons_hidden, num_iter, lmb):
    # Initialize weights and biases for the neural network
    p_hidden = npr.randn(num_neurons_hidden, 2)
    p_output = npr.randn(1, num_neurons_hidden + 1)  # Include bias

    # Combine parameters for hidden and output layers
    params = [p_hidden, p_output]

    # Find the gradient of the cost function
    cost_function_grad = grad(cost_function)

    # Gradient descent to update parameters
    for i in range(num_iter):
        # Compute gradients
        grad_params = cost_function_grad(params, t)

        # Update each parameter array separately
        for j in range(len(params)):
            params[j] -= lmb * grad_params[j]

    return params


# Analytical solution to the ODE
def g_analytic(t, P0=3, r=1):
    return P0 * np.exp(r * t) - 2 / r

# Main function to solve the given problem
if __name__ == '__main__':
    # Set seed for reproducibility
    npr.seed(15)

    # Define the time points for solving the ODE
    N = 100
    t = np.linspace(0, 1, N)

    # Set up initial parameters
    num_hidden_neurons = 10
    num_iter = 10000
    lmb = 0.001

    # Solve the ODE using the neural network
    params = solve_ode_neural_network(t, num_hidden_neurons, num_iter, lmb)

    # Analytical solution
    P_analytical = g_analytic(t)

    # Trial solution using the neural network
    P_nn = g_trial(t, params)

    # Plot the results
    plt.figure(figsize=(4, 4))
    plt.plot(t, P_analytical, label='Analytical')
    plt.plot(t, P_nn[0], label='Neural Network')
    plt.xlabel('t')
    plt.ylabel('P(t)')
    plt.title('Comparison of Analytical and Neural Network Solutions')
    plt.legend()
    plt.grid(True)
    plt.show()

#financial math probem 2

#import the libararies
import autograd.numpy as np
from autograd import elementwise_grad
import autograd.numpy.random as npr
from matplotlib import pyplot as plt

#defining AF
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

#Defining NN
def neural_network(params, x):
    w_hidden = params[0]
    w_output = params[1]

    num_values = np.size(x)
    x = x.reshape(-1, num_values)

    x_input = x

    x_input = np.concatenate((np.ones((1, num_values)), x_input), axis=0)

    z_hidden = np.matmul(w_hidden, x_input)
    x_hidden = sigmoid(z_hidden)

    x_hidden = np.concatenate((np.ones((1, num_values)), x_hidden), axis=0)

    z_output = np.matmul(w_output, x_hidden)
    x_output = z_output

    return x_output

# RHS of the non-homogeneous ODE
def g(t, L):
    k = 0.5  # Given constant k
    return -k * L + 2

# Trial sol using the neural network
def g_trial(t, params, L0=2):
    return L0 + t * neural_network(params, t)

#cost function
def cost_function(params, t):
    L_t = g_trial(t, params)
    d_L_t = elementwise_grad(g_trial, 0)(t, params)
    func = g(t, L_t)
    err_sqr = (d_L_t - func)**2
    return np.mean(err_sqr)

# Solve the ODE
def solve_ode_neural_network(t, num_neurons_hidden, num_iter, lmb):
    p_hidden = npr.randn(num_neurons_hidden, 2)
    p_output = npr.randn(1, num_neurons_hidden + 1)

    params = [p_hidden, p_output]

    cost_function_grad = grad(cost_function)

    for i in range(num_iter):
        grad_params = cost_function_grad(params, t)
        for j in range(len(params)):
            params[j] -= lmb * grad_params[j]

    return params

# Analytical solution
def L_analytic(t, L0=2, k=0.5):
    return (2 / k) + (L0 - (2 / k)) * np.exp(-k * t)

# Main function to solve the given problem
if __name__ == '__main__':
    npr.seed(15)

    N = 100
    t = np.linspace(0, 1, N)

    num_hidden_neurons = 10
    num_iter = 10000
    lmb = 0.001

    params = solve_ode_neural_network(t, num_hidden_neurons, num_iter, lmb)

    L_analytical = L_analytic(t)
    L_nn = g_trial(t, params)

    plt.figure(figsize=(4, 4))
    plt.plot(t, L_analytical, label='Analytical')
    plt.plot(t, L_nn[0], label='Neural Network')
    plt.xlabel('t')
    plt.ylabel('L(t)')
    plt.title('Comparison of Analytical and Neural Network Solutions')
    plt.legend()
    plt.grid(True)
    plt.show()

import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras import layers,models

#taking the input
from keras.datasets import mnist
(x_train,label_train),(x_test,label_test)=keras.datasets.mnist.load_data()

shape=x_train.shape
print(shape)
shape2=label_train.shape
print(shape2)

model=models.Sequential([
    layers.Dense(784,activation='sigmoid',input_shape=shape),
    layers.Dense(70,activation='sigmoid'),
    layers.Dense(10,activation='sigmoid')
])

model.compile(optimizer='adam',
              )