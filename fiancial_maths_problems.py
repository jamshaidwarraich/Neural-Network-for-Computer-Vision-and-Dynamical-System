# -*- coding: utf-8 -*-
"""Fiancial maths problems.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NAOoKnOKiBtV10BsO1h3I5JY2VgWGp3V
"""

#financial math problems 1st equation
import autograd.numpy as np
from autograd import grad, elementwise_grad
import autograd.numpy.random as npr
from matplotlib import pyplot as plt

def sigmoid(z):
  return 1/(1+np.exp(-z))

def neural_network(params, x):
    # Find the weights (including and biases) for the hidden and output layer.
    # Assume that params is a list of parameters for each layer.
    # The biases are the first element for each array in params,
    # and the weights are the remaining elements in each array in params.

    w_hidden = params[0]
    w_output = params[1]

    # Assumes input x being a one-dimensional array
    num_values = np.size(x)
    x = x.reshape(-1, num_values)

    # Assume that the input layer does nothing to the input x
    x_input = x

    ## Hidden layer:

    # Add a row of ones to include bias
    x_input = np.concatenate((np.ones((1, num_values)), x_input), axis=0)

    z_hidden = np.matmul(w_hidden, x_input)
    x_hidden = sigmoid(z_hidden)

    ## Output layer:

    # Include bias:
    x_hidden = np.concatenate((np.ones((1, num_values)), x_hidden), axis=0)

    z_output = np.matmul(w_output, x_hidden)
    x_output = z_output

    return x_output


# Define the right-hand side of the non-homogeneous ODE
def g(t, P):
    r = 1  # Given constant r
    return r * P + 2

# Define the trial solution using the neural network
def g_trial(t, params, P0=3):
    return P0 + t * neural_network(params, t)

# Define the cost function
def cost_function(params, t):
    g_t = g_trial(t, params)
    d_g_t = elementwise_grad(g_trial, 0)(t, params)
    func = g(t, g_t)
    err_sqr = (d_g_t - func)**2
    return np.mean(err_sqr)

def solve_ode_neural_network(t, num_neurons_hidden, num_iter, lmb):
    # Initialize weights and biases for the neural network
    p_hidden = npr.randn(num_neurons_hidden, 2)
    p_output = npr.randn(1, num_neurons_hidden + 1)  # Include bias

    # Combine parameters for hidden and output layers
    params = [p_hidden, p_output]

    # Find the gradient of the cost function
    cost_function_grad = grad(cost_function)

    # Gradient descent to update parameters
    for i in range(num_iter):
        # Compute gradients
        grad_params = cost_function_grad(params, t)

        # Update each parameter array separately
        for j in range(len(params)):
            params[j] -= lmb * grad_params[j]

    return params


# Analytical solution to the ODE
def g_analytic(t, P0=3, r=1):
    return P0 * np.exp(r * t) - 2 / r

# Main function to solve the given problem
if __name__ == '__main__':
    # Set seed for reproducibility
    npr.seed(15)

    # Define the time points for solving the ODE
    N = 100
    t = np.linspace(0, 1, N)

    # Set up initial parameters
    num_hidden_neurons = 10
    num_iter = 10000
    lmb = 0.001

    # Solve the ODE using the neural network
    params = solve_ode_neural_network(t, num_hidden_neurons, num_iter, lmb)

    # Analytical solution
    P_analytical = g_analytic(t)

    # Trial solution using the neural network
    P_nn = g_trial(t, params)

    # Plot the results
    plt.figure(figsize=(4, 4))
    plt.plot(t, P_analytical, label='Analytical')
    plt.plot(t, P_nn[0], label='Neural Network')
    plt.xlabel('t')
    plt.ylabel('P(t)')
    plt.title('Comparison of Analytical and Neural Network Solutions')
    plt.legend()
    plt.grid(True)
    plt.show()

#financial math probem 2

#import the libararies
import autograd.numpy as np
from autograd import elementwise_grad
import autograd.numpy.random as npr
from matplotlib import pyplot as plt

#defining AF
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

#Defining NN
def neural_network(params, x):
    w_hidden = params[0]
    w_output = params[1]

    num_values = np.size(x)
    x = x.reshape(-1, num_values)

    x_input = x

    x_input = np.concatenate((np.ones((1, num_values)), x_input), axis=0)

    z_hidden = np.matmul(w_hidden, x_input)
    x_hidden = sigmoid(z_hidden)

    x_hidden = np.concatenate((np.ones((1, num_values)), x_hidden), axis=0)

    z_output = np.matmul(w_output, x_hidden)
    x_output = z_output

    return x_output

# RHS of the non-homogeneous ODE
def g(t, L):
    k = 0.5  # Given constant k
    return -k * L + 2

# Trial sol using the neural network
def g_trial(t, params, L0=2):
    return L0 + t * neural_network(params, t)

#cost function
def cost_function(params, t):
    L_t = g_trial(t, params)
    d_L_t = elementwise_grad(g_trial, 0)(t, params)
    func = g(t, L_t)
    err_sqr = (d_L_t - func)**2
    return np.mean(err_sqr)

# Solve the ODE
def solve_ode_neural_network(t, num_neurons_hidden, num_iter, lmb):
    p_hidden = npr.randn(num_neurons_hidden, 2)
    p_output = npr.randn(1, num_neurons_hidden + 1)

    params = [p_hidden, p_output]

    cost_function_grad = grad(cost_function)

    for i in range(num_iter):
        grad_params = cost_function_grad(params, t)
        for j in range(len(params)):
            params[j] -= lmb * grad_params[j]

    return params

# Analytical solution
def L_analytic(t, L0=2, k=0.5):
    return (2 / k) + (L0 - (2 / k)) * np.exp(-k * t)

# Main function to solve the given problem
if __name__ == '__main__':
    npr.seed(15)

    N = 100
    t = np.linspace(0, 1, N)

    num_hidden_neurons = 10
    num_iter = 10000
    lmb = 0.001

    params = solve_ode_neural_network(t, num_hidden_neurons, num_iter, lmb)

    L_analytical = L_analytic(t)
    L_nn = g_trial(t, params)

    plt.figure(figsize=(4, 4))
    plt.plot(t, L_analytical, label='Analytical')
    plt.plot(t, L_nn[0], label='Neural Network')
    plt.xlabel('t')
    plt.ylabel('L(t)')
    plt.title('Comparison of Analytical and Neural Network Solutions')
    plt.legend()
    plt.grid(True)
    plt.show()

# Financial maths problem 3
import autograd.numpy as np
from autograd import grad, elementwise_grad
import autograd.numpy.random as npr
from matplotlib import pyplot as plt

def sigmoid(z):
    return 1/(1 + np.exp(-z))

def get_parameters():
    s = 0.0005
    c = 2
    gamma = 7
    g0 = 0.09
    return s, c, gamma, g0

# Assuming one input, hidden, and output layer
def neural_network(params, x):
    w_hidden = params[0]
    w_output = params[1]

    num_values = np.size(x)
    x = x.reshape(-1, num_values)

    x_input = x

    x_input = np.concatenate((np.ones((1, num_values)), x_input), axis=0)

    z_hidden = np.matmul(w_hidden, x_input)
    x_hidden = sigmoid(z_hidden)

    x_hidden = np.concatenate((np.ones((1, num_values)), x_hidden), axis=0)

    z_output = np.matmul(w_output, x_hidden)
    x_output = z_output

    return x_output

def g_trial(x, params, g0=0.09):
    return g0 + x * neural_network(params, x)


def g(x, g_trial):
    s, c, gamma, g0 = get_parameters()
    return g_trial * (s * c - gamma)


def cost_function(P, x):
    g_t = g_trial(x, P)
    d_net_out = elementwise_grad(neural_network, 1)(P, x)
    d_g_t = elementwise_grad(g_trial, 0)(x, P)
    func = g(x, g_t)
    err_sqr = (d_g_t - func)**2
    cost_sum = np.sum(err_sqr)
    return cost_sum / np.size(err_sqr)


def solve_ode_neural_network(x, num_neurons_hidden, num_iter, lmb):
    p0 = npr.randn(num_neurons_hidden, 2)
    p1 = npr.randn(1, num_neurons_hidden + 1)

    P = [p0, p1]

    print('Initial cost: %g' % cost_function(P, x))

    cost_function_grad = grad(cost_function, 0)

    for i in range(num_iter):
        cost_grad = cost_function_grad(P, x)
        P[0] = P[0] - lmb * cost_grad[0]
        P[1] = P[1] - lmb * cost_grad[1]

    print('Final cost: %g' % cost_function(P, x))

    return P


def g_analytic(t):
    s, c, gamma, g0 = get_parameters()
    return g0 * np.exp((s * c - gamma) * t)

if __name__ == '__main__':
    npr.seed(15)

    N = 10
    x = np.linspace(0, 1, N)

    num_hidden_neurons = 10
    num_iter = 10000
    lmb = 0.001

    P = solve_ode_neural_network(x, num_hidden_neurons, num_iter, lmb)

    res = g_trial(x, P)
    res_analytical = g_analytic(x)

    print('Max absolute difference: %g' % np.max(np.abs(res - res_analytical)))

    plt.figure(figsize=(8, 6))
    plt.title('Performance of neural network solving an ODE compared to the analytical solution')
    plt.plot(x, res_analytical)
    plt.plot(x, res[0, :])
    plt.legend(['Analytical', 'NN'])
    plt.xlabel('x')
    plt.ylabel('g(x)')
    plt.show()

# Financial math problem 4
import autograd.numpy as np
from autograd import grad, elementwise_grad
import autograd.numpy.random as npr
from matplotlib import pyplot as plt

def sigmoid(z):
    return 1/(1 + np.exp(-z))

def get_parameters():
    r = 0.0005
    n = 2
    g0 = 0.9
    return r, n, g0

# Assuming one input, hidden, and output layer
def neural_network(params, x):
    w_hidden = params[0]
    w_output = params[1]

    num_values = np.size(x)
    x = x.reshape(-1, num_values)

    x_input = x

    x_input = np.concatenate((np.ones((1, num_values)), x_input), axis=0)

    z_hidden = np.matmul(w_hidden, x_input)
    x_hidden = sigmoid(z_hidden)

    x_hidden = np.concatenate((np.ones((1, num_values)), x_hidden), axis=0)

    z_output = np.matmul(w_output, x_hidden)
    x_output = z_output

    return x_output

def g_trial(x, params, g0=0.9):
    return g0 + x * neural_network(params, x)

def g(x, g_trial):
    r, n, g0 = get_parameters()
    return g_trial * (r + n)

def cost_function(P, x):
    g_t = g_trial(x, P)
    d_net_out = elementwise_grad(neural_network, 1)(P, x)
    d_g_t = elementwise_grad(g_trial, 0)(x, P)
    func = g(x, g_t)
    err_sqr = (d_g_t - func)**2
    cost_sum = np.sum(err_sqr)
    return cost_sum / np.size(err_sqr)

def solve_ode_neural_network(x, num_neurons_hidden, num_iter, lmb):
    p0 = npr.randn(num_neurons_hidden, 2)
    p1 = npr.randn(1, num_neurons_hidden + 1)

    P = [p0, p1]

    print('Initial cost: %g' % cost_function(P, x))

    cost_function_grad = grad(cost_function, 0)

    for i in range(num_iter):
        cost_grad = cost_function_grad(P, x)
        P[0] = P[0] - lmb * cost_grad[0]
        P[1] = P[1] - lmb * cost_grad[1]

    print('Final cost: %g' % cost_function(P, x))

    return P

def g_analytic(t):
    r, n, g0 = get_parameters()
    return g0 * np.exp((r + n) * t)

if __name__ == '__main__':
    npr.seed(15)

    N = 10
    x = np.linspace(0, 1, N)

    num_hidden_neurons = 10
    num_iter = 10000
    lmb = 0.1

    P = solve_ode_neural_network(x, num_hidden_neurons, num_iter, lmb)

    res = g_trial(x, P)
    res_analytical = g_analytic(x)

    print('Max absolute difference: %g' % np.max(np.abs(res - res_analytical)))

    plt.figure(figsize=(8, 6))
    plt.title('Performance of neural network solving an ODE compared to the analytical solution')
    plt.plot(x, res_analytical)
    plt.plot(x, res[0, :])
    plt.legend(['Analytical', 'NN'])
    plt.xlabel('x')
    plt.ylabel('g(x)')
    plt.show()